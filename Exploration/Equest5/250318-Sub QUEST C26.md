# í”„ë¡œì íŠ¸: í•œêµ­ì–´ ë°ì´í„°ë¡œ ì±—ë´‡ ë§Œë“¤ê¸°

# ëª©ì°¨
- Step 1. ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°
- Step 2. ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°
- Step 3. SubwordTextEncoder ì‚¬ìš©í•˜ê¸°
- Step 4. ëª¨ë¸ êµ¬ì„±í•˜ê¸°
- Step 5. ëª¨ë¸ í‰ê°€í•˜ê¸°


```python
# ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í™•ì¸
import tensorflow as tf
import tensorflow_datasets as tfds
import os
import re
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

print(tf.__version__)
```

    2.6.0


## Step 1. ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°


```python
# ë°ì´í„° ê²½ë¡œ ì„¤ì •
data_path = os.path.expanduser('~/aiffel/transformer_chatbot/data/ChatbotData.csv')

print(os.path.exists(data_path))  # Trueê°€ ì¶œë ¥ë˜ì–´ì•¼ ì •ìƒ!
```

    True



```python
# ì‚¬ìš©í•  ìƒ˜í”Œì˜ ìµœëŒ€ ê°œìˆ˜
MAX_SAMPLES = 10000
print("ìµœëŒ€ ìƒ˜í”Œ ìˆ˜:", MAX_SAMPLES)
```

    ìµœëŒ€ ìƒ˜í”Œ ìˆ˜: 10000


## Step 2. ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°
- songys/Chatbot_dataëŠ” CSV íŒŒì¼ í•˜ë‚˜ë¡œ êµ¬ì„±ë˜ì–´ ìˆê³ , 'Q'(ì§ˆë¬¸)ì™€ 'A'(ë‹µë³€)ì´ ì´ë¯¸ ìŒìœ¼ë¡œ ì •ë¦¬ë˜ì–´ ìˆìŒ
- ì´ì— pandasë¡œ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í•œ ì¤„ì”© ë°”ë¡œ ì½ì–´ ì „ì²˜ë¦¬í•˜ì˜€ìŒ


```python
def preprocess_sentence(sentence):
    
    sentence = sentence.strip()  # ê³µë°± ì œê±°
    sentence = re.sub(r"([?.!,])", r" \1 ", sentence)  # êµ¬ë‘ì  ë¶„ë¦¬
    sentence = re.sub(r'[" "]+', " ", sentence)         # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    sentence = re.sub(r"[^ê°€-í£0-9?.!,]+", " ", sentence)  # í•œê¸€, ìˆ«ì, êµ¬ë‘ì  ì™¸ì˜ ë¬¸ìëŠ” ê³µë°±ìœ¼ë¡œ ëŒ€ì²´
    
    sentence = sentence.strip()
    return sentence
```


```python
# ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ì…‹ ë¡œë“œ í•¨ìˆ˜
def load_conversations():
    df = pd.read_csv(data_path)
    
    # ì§ˆë¬¸ê³¼ ë‹µë³€ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    inputs, outputs = [], []
    
    for idx, row in df.iterrows():
        if len(inputs) >= MAX_SAMPLES:
            break
        question, answer = row['Q'], row['A']
        inputs.append(preprocess_sentence(question))
        outputs.append(preprocess_sentence(answer))
    
    return inputs, outputs
```


```python
# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
questions, answers = load_conversations()

# ë°ì´í„° ìˆ˜ í™•ì¸
print('ì „ì²´ ìƒ˜í”Œ ìˆ˜:', len(questions))
print('ì „ì²´ ìƒ˜í”Œ ìˆ˜:', len(answers))
```

    ì „ì²´ ìƒ˜í”Œ ìˆ˜: 10000
    ì „ì²´ ìƒ˜í”Œ ìˆ˜: 10000



```python
# ì „ì²˜ë¦¬ ìƒ˜í”Œ í™•ì¸
print('ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(questions[21]))
print('ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(answers[21]))
```

    ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: ê°€ìŠ¤ë¹„ ì¥ë‚œ ì•„ë‹˜
    ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: ë‹¤ìŒ ë‹¬ì—ëŠ” ë” ì ˆì•½í•´ë´ìš” .


## Step 3. SubwordTextEncoder ì‚¬ìš©í•˜ê¸°

### ë‹¨ì–´ì¥(Vocabulary) ë§Œë“¤ê¸°


```python
# ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ Vocabulary ìƒì„±
tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)
```


```python
# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ê³ ìœ í•œ ì •ìˆ˜ ë¶€ì—¬í•˜ê¸°
START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]

# ë‹¨ì–´ì¥ í¬ê¸° (ì‹œì‘, ì¢…ë£Œ í† í° í¬í•¨)
VOCAB_SIZE = tokenizer.vocab_size + 2

print('START_TOKENì˜ ë²ˆí˜¸ :', START_TOKEN)
print('END_TOKENì˜ ë²ˆí˜¸ :', END_TOKEN)
print('ë‹¨ì–´ì¥ í¬ê¸° :', VOCAB_SIZE)
```

    START_TOKENì˜ ë²ˆí˜¸ : [8856]
    END_TOKENì˜ ë²ˆí˜¸ : [8857]
    ë‹¨ì–´ì¥ í¬ê¸° : 8858


### ê° ë‹¨ì–´ë¥¼ ê³ ìœ í•œ ì •ìˆ˜ë¡œ ì¸ì½”ë”©(Integer encoding) & íŒ¨ë”©(Padding)


```python
# ì„ì˜ì˜ 22ë²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•´ì„œ ì •ìˆ˜ ì¸ì½”ë”© ì‘ì—… ìˆ˜í–‰
# ê° í† í°ì„ ê³ ìœ í•œ ì •ìˆ˜ë¡œ ë³€í™˜
print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(tokenizer.encode(questions[21])))
print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(tokenizer.encode(answers[21])))
```

    ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: [8546, 3652, 7139]
    ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: [1761, 5697, 7, 4735, 115, 1]



```python
# ìƒ˜í”Œì˜ ìµœëŒ€ í—ˆìš© ê¸¸ì´
MAX_LENGTH = 40
print(MAX_LENGTH)
```

    40



```python
# ì •ìˆ˜ ì¸ì½”ë”©, ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ìƒ˜í”Œ ì œê±°, íŒ¨ë”©
def tokenize_and_filter(inputs, outputs):
    tokenized_inputs, tokenized_outputs = [], []
  
    for (sentence1, sentence2) in zip(inputs, outputs):
        # ì‹œì‘ê³¼ ì¢…ë£Œ í† í° ì¶”ê°€
        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN
        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN

        # ìµœëŒ€ ê¸¸ì´ ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ë§Œ ì €ì¥
        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:
            tokenized_inputs.append(sentence1)
            tokenized_outputs.append(sentence2)
  
    # íŒ¨ë”© ì ìš©
    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(
        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')

    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(
        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')

    return tokenized_inputs, tokenized_outputs
```


```python
# ë°ì´í„°ì…‹ í•„í„°ë§ ë° íŒ¨ë”© ì ìš©
questions, answers = tokenize_and_filter(questions, answers)

print('ë‹¨ì–´ì¥ì˜ í¬ê¸° :',(VOCAB_SIZE))
print('í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜:', len(questions))
print('í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜:', len(answers))
```

    ë‹¨ì–´ì¥ì˜ í¬ê¸° : 8858
    í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: 10000
    í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: 10000


### êµì‚¬ ê°•ìš”(Teacher Forcing) ì‚¬ìš©í•˜ê¸°


```python
BATCH_SIZE = 64
BUFFER_SIZE = 10000

# tf.data.Datasetìœ¼ë¡œ ë³€í™˜
dataset = tf.data.Dataset.from_tensor_slices((
    {
        'inputs': questions,
        'dec_inputs': answers[:, :-1] # ë””ì½”ë” ì…ë ¥
    },
    {
        'outputs': answers[:, 1:]     # ë””ì½”ë” ì¶œë ¥ (ë ˆì´ë¸”)
    },
))

# ì…”í”Œ ë° ë°°ì¹˜ ì ìš©
dataset = dataset.cache()
dataset = dataset.shuffle(BUFFER_SIZE)
dataset = dataset.batch(BATCH_SIZE)
dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)

print(dataset)
```

    <PrefetchDataset shapes: ({inputs: (None, 40), dec_inputs: (None, 39)}, {outputs: (None, 39)}), types: ({inputs: tf.int32, dec_inputs: tf.int32}, {outputs: tf.int32})>



```python
# ì„ì˜ì˜ ìƒ˜í”Œ í™•ì¸
print('ì •ìˆ˜ ì¸ì½”ë”©ëœ ì§ˆë¬¸ ìƒ˜í”Œ:', questions[11])
print('ì •ìˆ˜ ì¸ì½”ë”©ëœ ë‹µë³€ ìƒ˜í”Œ:', answers[11])

# ì •ìˆ˜ ì¸ì½”ë”©ëœ ìƒ˜í”Œì„ ë‹¤ì‹œ ë””ì½”ë”©í•˜ì—¬ í™•ì¸
print('ë””ì½”ë”©ëœ ì§ˆë¬¸:', tokenizer.decode([i for i in questions[11] if i not in [0, START_TOKEN[0], END_TOKEN[0]]])) # START_TOKENê³¼ END_TOKENì„ ì œì™¸í•œ í† í°ë§Œ ë””ì½”ë”©
print('ë””ì½”ë”©ëœ ë‹µë³€:', tokenizer.decode([i for i in answers[11] if i not in [0, START_TOKEN[0], END_TOKEN[0]]]))
```

    ì •ìˆ˜ ì¸ì½”ë”©ëœ ì§ˆë¬¸ ìƒ˜í”Œ: [8856  516  805 8857    0    0    0    0    0    0    0    0    0    0
        0    0    0    0    0    0    0    0    0    0    0    0    0    0
        0    0    0    0    0    0    0    0    0    0    0    0]
    ì •ìˆ˜ ì¸ì½”ë”©ëœ ë‹µë³€ ìƒ˜í”Œ: [8856   57  595  141    3    1 8857    0    0    0    0    0    0    0
        0    0    0    0    0    0    0    0    0    0    0    0    0    0
        0    0    0    0    0    0    0    0    0    0    0    0]
    ë””ì½”ë”©ëœ ì§ˆë¬¸: ê°€ë” ê¶ê¸ˆí•´
    ë””ì½”ë”©ëœ ë‹µë³€: ê·¸ ì‚¬ëŒë„ ê·¸ëŸ´ ê±°ì˜ˆìš” .


## Step 4. ëª¨ë¸ êµ¬ì„±í•˜ê¸°

### í¬ì§€ì…”ë„ ì¸ì½”ë”© (Positional Encoding)


```python
import tensorflow as tf

class PositionalEncoding(tf.keras.layers.Layer):
    def __init__(self, position, d_model, **kwargs):
        super(PositionalEncoding, self).__init__(**kwargs)
        self.position = position 
        self.d_model = d_model 
        self.pos_encoding = self.positional_encoding(position, d_model)

    def get_angles(self, position, i, d_model):
        # ê° ìœ„ì¹˜ì— ëŒ€í•œ ê°ë„ë¥¼ ê³„ì‚°
        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))
        return position * angles

    def positional_encoding(self, position, d_model):
        # ê°ë„ ë°°ì—´ ìƒì„±
        angle_rads = self.get_angles(
            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],
            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],
            d_model=d_model
        )
        # ë°°ì—´ì˜ ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” sin í•¨ìˆ˜ ì ìš©
        sines = tf.math.sin(angle_rads[:, 0::2])
        # ë°°ì—´ì˜ í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” cosine í•¨ìˆ˜ ì ìš©
        cosines = tf.math.cos(angle_rads[:, 1::2])
        
        # sinê³¼ cos ê°’ì„ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ë³‘í•©
        pos_encoding = tf.stack([sines, cosines], axis=0)
        pos_encoding = tf.transpose(pos_encoding, [1, 2, 0])
        pos_encoding = tf.reshape(pos_encoding, [position, d_model])
        
        pos_encoding = pos_encoding[tf.newaxis, ...]
        return tf.cast(pos_encoding, tf.float32)

    def call(self, inputs):
        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]

    def get_config(self):
        # HDF5 ì§ë ¬í™”ë¥¼ ìœ„í•œ ì„¤ì • ë°˜í™˜
        config = super(PositionalEncoding, self).get_config()
        config.update({
            "position": self.position,
            "d_model": self.d_model
        })
        return config

    @classmethod
    def from_config(cls, config):  # from_config() ì¶”ê°€ (ëª¨ë¸ ì¬ë¡œë”© ê°€ëŠ¥í•˜ê²Œ)
        return cls(**config)
```

### ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ (Scaled Dot Product Attention)


```python
def scaled_dot_product_attention(query, key, value, mask):
  # Qì™€ Kì˜ ë‚´ì 
  matmul_qk = tf.matmul(query, key, transpose_b=True)

  # ê°€ì¤‘ì¹˜ë¥¼ ì •ê·œí™” (ìŠ¤ì¼€ì¼ë§)
  depth = tf.cast(tf.shape(key)[-1], tf.float32)
  logits = matmul_qk / tf.math.sqrt(depth)

  # íŒ¨ë”©ì— ë§ˆìŠ¤í¬ ì¶”ê°€
  if mask is not None:
    logits += (mask * -1e9)

  # softmaxë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°
  attention_weights = tf.nn.softmax(logits, axis=-1)

  # ê°€ì¤‘ì¹˜ë¥¼ Vì— ì ìš©
  output = tf.matmul(attention_weights, value)
  return output
```

### ë©€í‹°í—¤ë“œ ì–´í…ì…˜ (Multi-Head Attention)


```python
class MultiHeadAttention(tf.keras.layers.Layer):

    def __init__(self, d_model, num_heads, **kwargs):
        super(MultiHeadAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.d_model = d_model

        assert d_model % self.num_heads == 0
        self.depth = d_model // self.num_heads

        self.query_dense = tf.keras.layers.Dense(units=d_model)
        self.key_dense = tf.keras.layers.Dense(units=d_model)
        self.value_dense = tf.keras.layers.Dense(units=d_model)

        self.dense = tf.keras.layers.Dense(units=d_model)

    def split_heads(self, inputs, batch_size):
        # ë©€í‹° í—¤ë“œë¡œ ë¶„ë¦¬
        inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(inputs, perm=[0, 2, 1, 3])


    def call(self, inputs):
        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']
        batch_size = tf.shape(query)[0]

        # Q, K, Vì— ê°ê° Dense ì ìš©
        query = self.query_dense(query)
        key = self.key_dense(key)
        value = self.value_dense(value)

        # ë³‘ë ¬ ì—°ì‚°ì„ ìœ„í•œ ë¨¸ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œ ë§Œë“¤ê¸°
        query = self.split_heads(query, batch_size)
        key = self.split_heads(key, batch_size)
        value = self.split_heads(value, batch_size)

        # ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜
        scaled_attention = scaled_dot_product_attention(query, key, value, mask)
        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])

        # ì–´í…ì…˜ ì—°ì‚° í›„ ê° ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì—°ê²°(concatenate)
        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))

        # ìµœì¢… ê²°ê³¼ì—ë„ Denseë¥¼ í•œ ë²ˆ ë” ì ìš©
        outputs = self.dense(concat_attention)
        return outputs

    def get_config(self):
        # HDF5 ì§ë ¬í™”ë¥¼ ìœ„í•œ ì„¤ì • ë°˜í™˜
        config = super(MultiHeadAttention, self).get_config()
        config.update({
            "d_model": self.d_model,
            "num_heads": self.num_heads
        })
        return config

    @classmethod
    def from_config(cls, config):
        return cls(**config)
```

### íŒ¨ë”© ë§ˆìŠ¤í‚¹(Padding Masking)


```python
def create_padding_mask(x):
  mask = tf.cast(tf.math.equal(x, 0), tf.float32)
  # (batch_size, 1, 1, sequence length)
  return mask[:, tf.newaxis, tf.newaxis, :]
```


```python
def create_look_ahead_mask(x):
  seq_len = tf.shape(x)[1]
  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)
  padding_mask = create_padding_mask(x)
  return tf.maximum(look_ahead_mask, padding_mask)
```

### ì¸ì½”ë”


```python
# ì¸ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„
# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ë‘ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬
def encoder_layer(units, d_model, num_heads, dropout, name="encoder_layer"):
  inputs = tf.keras.Input(shape=(None, d_model), name="inputs")

  # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©
  padding_mask = tf.keras.Input(shape=(1, 1, None), name="padding_mask")

  # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)
  attention = MultiHeadAttention(d_model, num_heads, name="attention")({
          'query': inputs,
          'key': inputs,
          'value': inputs,
          'mask': padding_mask
      })

  # ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” Dropoutê³¼ Layer Normalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ ìˆ˜í–‰
  attention = tf.keras.layers.Dropout(rate=dropout)(attention)
  attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)

  # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ
  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)
  outputs = tf.keras.layers.Dense(units=d_model)(outputs)

  # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ ìˆ˜í–‰
  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outputs)

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name)
```


```python
def encoder(vocab_size,
            num_layers,
            units,
            d_model,
            num_heads,
            dropout,
            name="encoder"):
  inputs = tf.keras.Input(shape=(None,), name="inputs")

  # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©
  padding_mask = tf.keras.Input(shape=(1, 1, None), name="padding_mask")

  # ì„ë² ë”© ë ˆì´ì–´
  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))

  # í¬ì§€ì…”ë„ ì¸ì½”ë”©
  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)

  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)

  # num_layersë§Œí¼ ìŒ“ì•„ì˜¬ë¦° ì¸ì½”ë”ì˜ ì¸µ.
  for i in range(num_layers):
    outputs = encoder_layer(
        units=units,
        d_model=d_model,
        num_heads=num_heads,
        dropout=dropout,
        name="encoder_layer_{}".format(i),
    )([outputs, padding_mask])

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name)
```

### ë””ì½”ë”


```python
# ë””ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.
# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ì„¸ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬
def decoder_layer(units, d_model, num_heads, dropout, name="decoder_layer"):
  inputs = tf.keras.Input(shape=(None, d_model), name="inputs")
  enc_outputs = tf.keras.Input(shape=(None, d_model), name="encoder_outputs")
  look_ahead_mask = tf.keras.Input(
      shape=(1, None, None), name="look_ahead_mask")
  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')

  # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)
  attention1 = MultiHeadAttention(
      d_model, num_heads, name="attention_1")(inputs={
          'query': inputs,
          'key': inputs,
          'value': inputs,
          'mask': look_ahead_mask
      })

  # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ ìˆ˜í–‰
  attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)

  # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜)
  attention2 = MultiHeadAttention(
      d_model, num_heads, name="attention_2")(inputs={
          'query': attention1,
          'key': enc_outputs,
          'value': enc_outputs,
          'mask': padding_mask
      })

  # ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ”
  # Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ ìˆ˜í–‰
  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)
  attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)

  # ì„¸ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ
  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)
  outputs = tf.keras.layers.Dense(units=d_model)(outputs)

  # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalization ìˆ˜í–‰
  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)

  return tf.keras.Model(
      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],
      outputs=outputs,
      name=name)
```


```python
def decoder(vocab_size,
            num_layers,
            units,
            d_model,
            num_heads,
            dropout,
            name='decoder'):
  inputs = tf.keras.Input(shape=(None,), name='inputs')
  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')
  look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')

  # íŒ¨ë”© ë§ˆìŠ¤í¬
  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')
  
  # ì„ë² ë”© ë ˆì´ì–´
  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))

  # í¬ì§€ì…”ë„ ì¸ì½”ë”©
  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)

  # Dropoutì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰
  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)

  for i in range(num_layers):
    outputs = decoder_layer(
        units=units,
        d_model=d_model,
        num_heads=num_heads,
        dropout=dropout,
        name='decoder_layer_{}'.format(i),
    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])

  return tf.keras.Model(
      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],
      outputs=outputs,
      name=name)
```

### ì „ì²´ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ êµ¬ì„±


```python
def transformer(vocab_size,
                num_layers,
                units,
                d_model,
                num_heads,
                dropout,
                name="transformer"):
  inputs = tf.keras.Input(shape=(None,), name="inputs")
  dec_inputs = tf.keras.Input(shape=(None,), name="dec_inputs")

  # ì¸ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬
  enc_padding_mask = tf.keras.layers.Lambda(
      create_padding_mask, output_shape=(1, 1, None),
      name='enc_padding_mask')(inputs)

  # ë””ì½”ë”ì—ì„œ ë¯¸ë˜ì˜ í† í°ì„ ë§ˆìŠ¤í¬ í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©
  # ë‚´ë¶€ì— íŒ¨ë”© ë§ˆìŠ¤í¬ë„ í¬í•¨
  look_ahead_mask = tf.keras.layers.Lambda(
      create_look_ahead_mask,
      output_shape=(1, None, None),
      name='look_ahead_mask')(dec_inputs)

  # ë‘ ë²ˆì§¸ ì–´í…ì…˜ ë¸”ë¡ì—ì„œ ì¸ì½”ë”ì˜ ë²¡í„°ë“¤ì„ ë§ˆìŠ¤í‚¹
  # ë””ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬
  dec_padding_mask = tf.keras.layers.Lambda(
      create_padding_mask, output_shape=(1, 1, None),
      name='dec_padding_mask')(inputs)

  # ì¸ì½”ë”
  enc_outputs = encoder(
      vocab_size=vocab_size,
      num_layers=num_layers,
      units=units,
      d_model=d_model,
      num_heads=num_heads,
      dropout=dropout,
  )(inputs=[inputs, enc_padding_mask])

  # ë””ì½”ë”
  dec_outputs = decoder(
      vocab_size=vocab_size,
      num_layers=num_layers,
      units=units,
      d_model=d_model,
      num_heads=num_heads,
      dropout=dropout,
  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])

  # ì™„ì „ì—°ê²°ì¸µ
  outputs = tf.keras.layers.Dense(units=vocab_size, name="outputs")(dec_outputs)

  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)
```

## Step 5. ëª¨ë¸ í‰ê°€í•˜ê¸°

### ëª¨ë¸ í›ˆë ¨


```python
tf.keras.backend.clear_session()

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
NUM_LAYERS = 2 # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì¸µì˜ ê°œìˆ˜
D_MODEL = 256 # ì¸ì½”ë”ì™€ ë””ì½”ë” ë‚´ë¶€ì˜ ì…, ì¶œë ¥ì˜ ê³ ì • ì°¨ì›
NUM_HEADS = 8 # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜ 
UNITS = 512 # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°
DROPOUT = 0.1 # ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨

model = transformer(
    vocab_size=VOCAB_SIZE,
    num_layers=NUM_LAYERS,
    units=UNITS,
    d_model=D_MODEL,
    num_heads=NUM_HEADS,
    dropout=DROPOUT)

model.summary()
```

    Model: "transformer"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    inputs (InputLayer)             [(None, None)]       0                                            
    __________________________________________________________________________________________________
    dec_inputs (InputLayer)         [(None, None)]       0                                            
    __________________________________________________________________________________________________
    enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     
    __________________________________________________________________________________________________
    encoder (Functional)            (None, None, 256)    3321856     inputs[0][0]                     
                                                                     enc_padding_mask[0][0]           
    __________________________________________________________________________________________________
    look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 
    __________________________________________________________________________________________________
    dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     
    __________________________________________________________________________________________________
    decoder (Functional)            (None, None, 256)    3849216     dec_inputs[0][0]                 
                                                                     encoder[0][0]                    
                                                                     look_ahead_mask[0][0]            
                                                                     dec_padding_mask[0][0]           
    __________________________________________________________________________________________________
    outputs (Dense)                 (None, None, 8858)   2276506     decoder[0][0]                    
    ==================================================================================================
    Total params: 9,447,578
    Trainable params: 9,447,578
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
# ì†ì‹¤ í•¨ìˆ˜(Loss function)
def loss_function(y_true, y_pred):
  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))
  
  loss = tf.keras.losses.SparseCategoricalCrossentropy(
      from_logits=True, reduction='none')(y_true, y_pred)

  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)
  loss = tf.multiply(loss, mask)

  return tf.reduce_mean(loss)
```


```python
# ì»¤ìŠ¤í…€ ëœ í•™ìŠµë¥ (Learning rate)
class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, d_model, warmup_steps=4000, **kwargs):
        super(CustomSchedule, self).__init__(**kwargs)
        self.d_model = d_model
        self.d_model = tf.cast(self.d_model, tf.float32)
        self.warmup_steps = warmup_steps

    def __call__(self, step):
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)
        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

    def get_config(self):  # get_config() ì¶”ê°€ (ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ)
        return {"d_model": int(self.d_model.numpy()), "warmup_steps": self.warmup_steps}

    @classmethod
    def from_config(cls, config):
        return cls(**config)

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ í™•ì¸
sample_learning_rate = CustomSchedule(d_model=128)

plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))
plt.ylabel("Learning Rate")
plt.xlabel("Train Step")
plt.show()
```


    
![png](output_45_0.png)
    



```python
# ëª¨ë¸ ì»´íŒŒì¼
learning_rate = CustomSchedule(D_MODEL)

optimizer = tf.keras.optimizers.Adam(
    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)

def accuracy(y_true, y_pred):
  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))
  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)

model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])
```


```python
# í›ˆë ¨í•˜ê¸°
EPOCHS = 10
model.fit(dataset, epochs=EPOCHS, verbose=1)
```

    Epoch 1/10
    157/157 [==============================] - 15s 54ms/step - loss: 1.4207 - accuracy: 0.0272
    Epoch 2/10
    157/157 [==============================] - 8s 54ms/step - loss: 1.1891 - accuracy: 0.0493
    Epoch 3/10
    157/157 [==============================] - 8s 54ms/step - loss: 0.9937 - accuracy: 0.0498
    Epoch 4/10
    157/157 [==============================] - 9s 55ms/step - loss: 0.9039 - accuracy: 0.0517
    Epoch 5/10
    157/157 [==============================] - 9s 54ms/step - loss: 0.8515 - accuracy: 0.0549
    Epoch 6/10
    157/157 [==============================] - 9s 55ms/step - loss: 0.8014 - accuracy: 0.0578
    Epoch 7/10
    157/157 [==============================] - 9s 55ms/step - loss: 0.7481 - accuracy: 0.0618
    Epoch 8/10
    157/157 [==============================] - 9s 56ms/step - loss: 0.6883 - accuracy: 0.0676
    Epoch 9/10
    157/157 [==============================] - 9s 56ms/step - loss: 0.6218 - accuracy: 0.0751
    Epoch 10/10
    157/157 [==============================] - 9s 56ms/step - loss: 0.5490 - accuracy: 0.0838





    <keras.callbacks.History at 0x7c1aa86acf10>




```python
import os
import pickle
import tensorflow as tf

# ì €ì¥ ê²½ë¡œ (HDF5 í¬ë§·)
model_save_path = os.path.expanduser("~/aiffel/transformer_chatbot/model/model.h5")

# ëª¨ë¸ ì €ì¥ (HDF5 í¬ë§·)
model.save(model_save_path)
print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")

# í† í¬ë‚˜ì´ì € ì €ì¥
tokenizer_save_path = os.path.expanduser("~/aiffel/transformer_chatbot/model/tokenizer.pickle")
with open(tokenizer_save_path, "wb") as f:
    pickle.dump(tokenizer, f)
print(f"âœ… í† í¬ë‚˜ì´ì € ì €ì¥ ì™„ë£Œ: {tokenizer_save_path}")
```

    âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: /aiffel/aiffel/transformer_chatbot/model/model.h5
    âœ… í† í¬ë‚˜ì´ì € ì €ì¥ ì™„ë£Œ: /aiffel/aiffel/transformer_chatbot/model/tokenizer.pickle


### ëª¨ë¸ í‰ê°€


```python
def decoder_inference(sentence):
  sentence = preprocess_sentence(sentence)

  # ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€
  sentence = tf.expand_dims(
      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)

  # ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜
  output_sequence = tf.expand_dims(START_TOKEN, 0)

  # ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„
  for i in range(MAX_LENGTH):
    # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ ë°˜ë³µ
    predictions = model(inputs=[sentence, output_sequence], training=False)
    predictions = predictions[:, -1:, :]

    # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜
    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)

    # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ
    if tf.equal(predicted_id, END_TOKEN[0]):
      break

    # ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€
    # ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë¨
    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)

  return tf.squeeze(output_sequence, axis=0)
```


```python
# ì„ì˜ì˜ ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ ì±—ë´‡ì˜ ëŒ€ë‹µì„ ì–»ëŠ”Â sentence_generation()Â í•¨ìˆ˜
def sentence_generation(sentence):
  # ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŒ
  prediction = decoder_inference(sentence)

  # ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
  predicted_sentence = tokenizer.decode(
      [i for i in prediction if i < tokenizer.vocab_size])

  print('ì…ë ¥ : {}'.format(sentence))
  print('ì¶œë ¥ : {}'.format(predicted_sentence))

  return predicted_sentence
```


```python
sentence_generation('ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?')
```

    ì…ë ¥ : ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?
    ì¶œë ¥ : ë§ì´ ì§€ì³¤ë‚˜ë´ìš” .





    'ë§ì´ ì§€ì³¤ë‚˜ë´ìš” .'




```python
sentence_generation("ì–´ì œëŠ” ì†ìƒí–ˆì–´.")
```

    ì…ë ¥ : ì–´ì œëŠ” ì†ìƒí–ˆì–´.
    ì¶œë ¥ : ì¢‹ì€ ì‚¬ëŒ ë§Œë‚˜ì„¸ìš” .





    'ì¢‹ì€ ì‚¬ëŒ ë§Œë‚˜ì„¸ìš” .'




```python
# Perplexity ê³„ì‚° í•¨ìˆ˜
def calculate_perplexity(model, dataset):
    total_loss = 0
    total_count = 0
    
    for batch in dataset:
        inputs, targets = batch
        predictions = model(inputs, training=False)
        
        # ì†ì‹¤ ê°’ ê³„ì‚°
        loss = loss_function(targets['outputs'], predictions)
        total_loss += loss.numpy()
        total_count += 1

    # Perplexity ê³„ì‚°
    perplexity = np.exp(total_loss / total_count)
    return perplexity

# Perplexity ì¸¡ì •
perplexity = calculate_perplexity(model, dataset)
print(f"\nâœ… Perplexity: {perplexity:.4f}") # 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì´ìƒì . 20~30ì´ë©´ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€
```

    
    âœ… Perplexity: 1.5692



```python
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

# BLEU ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜
def calculate_bleu_score(references, candidates):
    smoothie = SmoothingFunction().method4
    scores = []
    for ref, cand in zip(references, candidates):
        score = sentence_bleu([ref.split()], cand.split(), smoothing_function=smoothie)
        scores.append(score)
    return np.mean(scores)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
test_sentences = [
    "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?",
    "ê¸°ë¶„ì´ ì•ˆ ì¢‹ì•„.",
    "ì£¼ë§ì— ë­ í• ê¹Œ?",
    "ì•ˆë…•!"
]

# ì •ë‹µ ë¬¸ì¥ (ì§ì ‘ ì‘ì„±í•˜ê±°ë‚˜ í‰ê°€ìš© ë°ì´í„°ì…‹ ì‚¬ìš©)
reference_sentences = [
    "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ì•„ìš”.",
    "ê¸°ë¶„ì´ ì•ˆ ì¢‹êµ°ìš”. ë¬´ìŠ¨ ì¼ ìˆì—ˆë‚˜ìš”?",
    "ì£¼ë§ì— ì‚°ì±…ì´ë‚˜ ì˜í™” ë³´ëŠ” ê±´ ì–´ë•Œìš”?",
    "ì•ˆë…•í•˜ì„¸ìš”!"
]

# ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë¬¸ì¥
predicted_sentences = [sentence_generation(sentence) for sentence in test_sentences]

# BLEU ì ìˆ˜ ê³„ì‚°
bleu_score = calculate_bleu_score(reference_sentences, predicted_sentences)
print(f"\nâœ… BLEU Score: {bleu_score:.4f}") # 0.3 ì´ìƒì´ë©´ ê½¤ ì¢‹ìŒ
```

    ì…ë ¥ : ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?
    ì¶œë ¥ : ë©‹ì§„ ë§ì´ì—ìš” .
    ì…ë ¥ : ê¸°ë¶„ì´ ì•ˆ ì¢‹ì•„.
    ì¶œë ¥ : ì €ë„ ì‹¶ì–´ìš” .
    ì…ë ¥ : ì£¼ë§ì— ë­ í• ê¹Œ?
    ì¶œë ¥ : ê°™ì´ ê°€ë³´ì„¸ìš” .
    ì…ë ¥ : ì•ˆë…•!
    ì¶œë ¥ : ì €ë„ ì¢‹ì•„í•´ìš” .
    
    âœ… BLEU Score: 0.0000



```python
from rouge_score import rouge_scorer

# ROUGE ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜
def calculate_rouge_score(references, candidates):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    rouge1, rouge2, rougeL = [], [], []

    for ref, cand in zip(references, candidates):
        scores = scorer.score(ref, cand)
        rouge1.append(scores['rouge1'].fmeasure)
        rouge2.append(scores['rouge2'].fmeasure)
        rougeL.append(scores['rougeL'].fmeasure)

    return np.mean(rouge1), np.mean(rouge2), np.mean(rougeL)

# ROUGE ì ìˆ˜ ê³„ì‚°
rouge1, rouge2, rougeL = calculate_rouge_score(reference_sentences, predicted_sentences)
print(f"\nâœ… ROUGE-1: {rouge1:.4f}") # 0.5 ì´ìƒì´ë©´ ì–‘í˜¸
print(f"âœ… ROUGE-2: {rouge2:.4f}") # 0.3 ì´ìƒì´ë©´ ì–‘í˜¸
print(f"âœ… ROUGE-L: {rougeL:.4f}") # 0.4 ì´ìƒì´ë©´ ì–‘í˜¸
```

    
    âœ… ROUGE-1: 0.0000
    âœ… ROUGE-2: 0.0000
    âœ… ROUGE-L: 0.0000



```python
import os
import pickle
import tensorflow as tf

# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ê²½ë¡œ
model_path = os.path.expanduser("~/aiffel/transformer_chatbot/model/model.h5")
tokenizer_path = os.path.expanduser("~/aiffel/transformer_chatbot/model/tokenizer.pickle")

# âœ… ì»¤ìŠ¤í…€ ë ˆì´ì–´ ë“±ë¡ í›„ ëª¨ë¸ ë¡œë“œ
custom_objects = {
    "PositionalEncoding": PositionalEncoding,
    "MultiHeadAttention": MultiHeadAttention,
    "CustomSchedule": CustomSchedule
}
model = tf.keras.models.load_model(model_path, custom_objects=custom_objects, compile=False)

# í† í¬ë‚˜ì´ì € ë¡œë“œ
with open(tokenizer_path, "rb") as f:
    tokenizer = pickle.load(f)

# ì±—ë´‡ ì„¤ì •
START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]
MAX_LENGTH = 40

def chatbot():
    """ì‹¤ì œ ì±—ë´‡ ì‹¤í–‰"""
    while True:
        user_input = input("ğŸ§‘â€ğŸ’» ë‹¹ì‹ : ").strip()
        if user_input.lower() == "ì¢…ë£Œ":
            print("ğŸ”´ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ğŸ‘‹")
            break
        response = sentence_generation(user_input)
        print(f"ğŸ¤– ì±—ë´‡: {response}")

def sentence_generation(sentence):
    """ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•œ ì±—ë´‡ ì‘ë‹µ ìƒì„±"""
    sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(sentence.strip()) + END_TOKEN, axis=0)
    output = tf.expand_dims(START_TOKEN, 0)

    for _ in range(MAX_LENGTH):
        predictions = model([sentence, output], training=False)
        predicted_id = tf.argmax(predictions[:, -1:, :], axis=-1)
        
        # int64 â†’ int32 ë³€í™˜
        predicted_id = tf.cast(predicted_id, dtype=tf.int32)

        if tf.equal(predicted_id, END_TOKEN[0]):
            break
        output = tf.concat([output, predicted_id], axis=-1)

    return tokenizer.decode([i for i in tf.squeeze(output) if i < tokenizer.vocab_size])

if __name__ == "__main__":
    chatbot()
```

    ğŸ§‘â€ğŸ’» ë‹¹ì‹ : ì•ˆë…•?
    ğŸ¤– ì±—ë´‡: ì¢‹ì€ ê³³ìœ¼ë¡œ ê°€ë³´ì„¸ìš” .
    ğŸ§‘â€ğŸ’» ë‹¹ì‹ : ì–´ë””ë¡œ?
    ğŸ¤– ì±—ë´‡: ì €ë„ ì¢‹ì•„í•´ìš” .
    ğŸ§‘â€ğŸ’» ë‹¹ì‹ : ì™œ ì¢‹ì•„?
    ğŸ¤– ì±—ë´‡: ì €ë„ ê°™ì´ ê°€ë³´ì„¸ìš” .
    ğŸ§‘â€ğŸ’» ë‹¹ì‹ : ì–´ë”” ê°€ê³ ì‹¶ì–´?
    ğŸ¤– ì±—ë´‡: ì¢‹ì€ ì‚¬ëŒ ë§Œë‚˜ì„¸ìš” .
    ğŸ§‘â€ğŸ’» ë‹¹ì‹ : ì¢…ë£Œ
    ğŸ”´ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ğŸ‘‹


## ëª¨ë¸ ì„±ëŠ¥ ê°œì„ 
**ì‚¬ì „ í•™ìŠµ ëª¨ë¸(KETI-AIR/ke-t5-small) ì‚¬ìš©**

### í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜


```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
!pip install transformers datasets torch torchvision torchaudio fastapi uvicorn
!pip install rouge-score nltk
```

    Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (4.11.3)
    Requirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (1.14.0)
    Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (1.9.1+cu111)
    Requirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (0.10.1+cu111)
    Requirement already satisfied: torchaudio in /opt/conda/lib/python3.9/site-packages (0.9.1)
    Requirement already satisfied: fastapi in /opt/conda/lib/python3.9/site-packages (0.115.11)
    Requirement already satisfied: uvicorn in /opt/conda/lib/python3.9/site-packages (0.34.0)
    Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers) (3.4.0)
    Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers) (4.62.3)
    Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.10.3)
    Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (6.0)
    Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (1.21.4)
    Requirement already satisfied: sacremoses in /opt/conda/lib/python3.9/site-packages (from transformers) (0.0.46)
    Requirement already satisfied: huggingface-hub>=0.0.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.0.19)
    Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (21.3)
    Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (2021.11.10)
    Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers) (2.26.0)
    Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (2021.11.1)
    Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets) (3.8.1)
    Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets) (1.3.3)
    Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets) (2.0.2)
    Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets) (0.70.12.2)
    Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (6.0.1)
    Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from datasets) (0.3.4)
    Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch) (4.12.2)
    Requirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision) (8.3.2)
    Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /opt/conda/lib/python3.9/site-packages (from fastapi) (0.46.1)
    Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from fastapi) (2.10.6)
    Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.9/site-packages (from uvicorn) (0.9.0)
    Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from uvicorn) (8.0.3)
    Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.6)
    Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)
    Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)
    Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)
    Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (1.26.7)
    Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.10)
    Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.0.8)
    Requirement already satisfied: anyio<5,>=3.6.2 in /opt/conda/lib/python3.9/site-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)
    Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)
    Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)
    Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.1)
    Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)
    Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.7.2)
    Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (21.2.0)
    Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)
    Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2021.3)
    Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)
    Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)
    Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.9/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.2.0)
    Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.9/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.2.2)
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m
    Requirement already satisfied: rouge-score in /opt/conda/lib/python3.9/site-packages (0.1.2)
    Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.6.5)
    Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge-score) (1.21.4)
    Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge-score) (0.12.0)
    Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge-score) (1.16.0)
    Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.3)
    Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.1.0)
    Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2021.11.10)
    Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.62.3)
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m



```python
import os
import torch
import re
from datasets import load_dataset
from transformers import AutoTokenizer
```

### ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬


```python
# GPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
```

    Using device: cuda



```python
# ì‚¬ì „í•™ìŠµëœ í† í¬ë‚˜ì´ì € ë¡œë“œ (KE-T5 ëª¨ë¸ ì‚¬ìš©)
model_name = 'KETI-AIR/ke-t5-small'
tokenizer = AutoTokenizer.from_pretrained(model_name)
```


```python
# ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • ë° ë¡œë“œ
data_path = os.path.expanduser('~/aiffel/transformer_chatbot/data/ChatbotData.csv')

# íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
if not os.path.exists(data_path):
    raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
    
# ë°ì´í„°ì…‹ ë¡œë“œ (csv í˜•ì‹ ì§€ì •)
dataset = load_dataset('csv', data_files={'train': data_path})
```

    WARNING:datasets.builder:Using custom data configuration default-382c7ca4bfe493c3
    WARNING:datasets.builder:Reusing dataset csv (/aiffel/.cache/huggingface/datasets/csv/default-382c7ca4bfe493c3/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)



      0%|          | 0/1 [00:00<?, ?it/s]



```python
# ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •
MAX_LENGTH = 40  

# ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_function(examples):
    # ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶”ì¶œ
    inputs = [q for q in examples['Q']]
    targets = [a for a in examples['A']]
    
    # ì…ë ¥ê³¼ ì¶œë ¥ ëª¨ë‘ í† í¬ë‚˜ì´ì¦ˆ
    model_inputs = tokenizer(inputs, max_length=MAX_LENGTH, truncation=True, padding='max_length')

    # íƒ€ê²Ÿ(ë‹µë³€)ë„ í† í¬ë‚˜ì´ì¦ˆ
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=MAX_LENGTH, truncation=True, padding='max_length')

    # ëª¨ë¸ ì…ë ¥ì— ë ˆì´ë¸” ì¶”ê°€
    model_inputs['labels'] = labels['input_ids']
    return model_inputs

# ë°ì´í„°ì…‹ì— ì „ì²˜ë¦¬ ì ìš©
tokenized_dataset = dataset.map(preprocess_function, batched=True)

print(tokenized_dataset)
```


      0%|          | 0/12 [00:00<?, ?ba/s]


    DatasetDict({
        train: Dataset({
            features: ['A', 'Q', 'attention_mask', 'input_ids', 'label', 'labels'],
            num_rows: 11823
        })
    })


### ë°ì´í„°ì…‹ ë¶„í•  ë° DataLoader ìƒì„±


```python
from torch.utils.data import DataLoader

# 'train' ë°ì´í„°ì…‹ ì„ íƒ
dataset = tokenized_dataset['train']

# ë°ì´í„°ì…‹ ë¶„í•  (90% í•™ìŠµ, 10% í‰ê°€)
train_size = int(0.9 * len(dataset))
train_dataset = dataset.select(range(train_size))
eval_dataset = dataset.select(range(train_size, len(dataset)))

# PyTorch DataLoaderë¥¼ ìœ„í•´ í˜•ì‹ ë³€í™˜
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# DataLoader ìƒì„±
BATCH_SIZE = 16
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE)

print(f"Train DataLoader: {len(train_dataloader)} batches")
print(f"Eval DataLoader: {len(eval_dataloader)} batches")
```

    Train DataLoader: 665 batches
    Eval DataLoader: 74 batches


### ëª¨ë¸ í•™ìŠµ ë° í‰ê°€


```python
from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments

# ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

# í•™ìŠµ ì¸ì ì„¤ì •
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    save_total_limit=2,
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
)

# Trainer ìƒì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

# ëª¨ë¸ í•™ìŠµ
trainer.train()
```

    The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: A, Q.
    ***** Running training *****
      Num examples = 10640
      Num Epochs = 3
      Instantaneous batch size per device = 16
      Total train batch size (w. parallel, distributed & accumulation) = 16
      Gradient Accumulation steps = 1
      Total optimization steps = 1995




    <div>

      <progress value='1995' max='1995' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [1995/1995 05:06, Epoch 3/3]
    </div>
    <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>15.730100</td>
      <td>10.907944</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.837300</td>
      <td>1.973964</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.686800</td>
      <td>1.822395</td>
    </tr>
  </tbody>
</table><p>


    The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: A, Q.
    ***** Running Evaluation *****
      Num examples = 1183
      Batch size = 16
    Saving model checkpoint to ./results/checkpoint-665
    Configuration saved in ./results/checkpoint-665/config.json
    Model weights saved in ./results/checkpoint-665/pytorch_model.bin
    Deleting older checkpoint [results/checkpoint-1330] due to args.save_total_limit
    The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: A, Q.
    ***** Running Evaluation *****
      Num examples = 1183
      Batch size = 16
    Saving model checkpoint to ./results/checkpoint-1330
    Configuration saved in ./results/checkpoint-1330/config.json
    Model weights saved in ./results/checkpoint-1330/pytorch_model.bin
    Deleting older checkpoint [results/checkpoint-1995] due to args.save_total_limit
    The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: A, Q.
    ***** Running Evaluation *****
      Num examples = 1183
      Batch size = 16
    Saving model checkpoint to ./results/checkpoint-1995
    Configuration saved in ./results/checkpoint-1995/config.json
    Model weights saved in ./results/checkpoint-1995/pytorch_model.bin
    Deleting older checkpoint [results/checkpoint-665] due to args.save_total_limit
    
    
    Training completed. Do not forget to share your model on huggingface.co/models =)
    
    
    Loading best model from ./results/checkpoint-1995 (score: 1.8223949670791626).





    TrainOutput(global_step=1995, training_loss=14.767111160163592, metrics={'train_runtime': 306.8553, 'train_samples_per_second': 104.023, 'train_steps_per_second': 6.501, 'total_flos': 337547840716800.0, 'train_loss': 14.767111160163592, 'epoch': 3.0})



### í•™ìŠµëœ ëª¨ë¸ ì €ì¥


```python
# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥
model.save_pretrained("./finetuned_model")
tokenizer.save_pretrained("./finetuned_model")
```

    Configuration saved in ./finetuned_model/config.json
    Model weights saved in ./finetuned_model/pytorch_model.bin
    tokenizer config file saved in ./finetuned_model/tokenizer_config.json
    Special tokens file saved in ./finetuned_model/special_tokens_map.json
    Copy vocab file to ./finetuned_model/spiece.model





    ('./finetuned_model/tokenizer_config.json',
     './finetuned_model/special_tokens_map.json',
     './finetuned_model/spiece.model',
     './finetuned_model/added_tokens.json',
     './finetuned_model/tokenizer.json')



### ëª¨ë¸ í‰ê°€


```python
import torch

# ëª¨ë¸ì„ GPUë¡œ ì´ë™
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "./finetuned_model"  # ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

MAX_LENGTH = 40  # ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´


def generate_response(input_text):
    """KE-T5 ëª¨ë¸ì„ ì‚¬ìš©í•´ ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±"""
    # ì…ë ¥ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì¦ˆ ë° GPU ì´ë™
    inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)

    # ì‘ë‹µ ìƒì„±
    outputs = model.generate(inputs, max_length=MAX_LENGTH, num_beams=5, early_stopping=True)

    # ìƒì„±ëœ í† í°ì„ ë¬¸ì¥ìœ¼ë¡œ ë””ì½”ë”©
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


# ì±—ë´‡ ì‹¤í–‰ í•¨ìˆ˜
def chatbot():
    print("ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤. 'ì¢…ë£Œ'ë¼ê³  ì…ë ¥í•˜ë©´ ëŒ€í™”ê°€ ì¢…ë£Œë©ë‹ˆë‹¤.")

    while True:
        user_input = input("ë‹¹ì‹ : ")

        if user_input.strip() == "ì¢…ë£Œ":
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”!")
            break

        response = generate_response(user_input)
        print(f"ì±—ë´‡: {response}")


# ì±—ë´‡ ì‹¤í–‰
if __name__ == "__main__":
    chatbot()
```

    Didn't find file ./finetuned_model/added_tokens.json. We won't load it.
    loading file ./finetuned_model/spiece.model
    loading file ./finetuned_model/tokenizer.json
    loading file None
    loading file ./finetuned_model/special_tokens_map.json
    loading file ./finetuned_model/tokenizer_config.json
    loading configuration file ./finetuned_model/config.json
    Model config T5Config {
      "_name_or_path": "KETI-AIR/ke-t5-small",
      "architectures": [
        "T5ForConditionalGeneration"
      ],
      "d_ff": 1024,
      "d_kv": 64,
      "d_model": 512,
      "decoder_start_token_id": 0,
      "dropout_rate": 0.0,
      "eos_token_id": 1,
      "feed_forward_proj": "gated-gelu",
      "initializer_factor": 1.0,
      "is_encoder_decoder": true,
      "layer_norm_epsilon": 1e-06,
      "model_type": "t5",
      "n_positions": 512,
      "num_decoder_layers": 8,
      "num_heads": 6,
      "num_layers": 8,
      "pad_token_id": 0,
      "relative_attention_num_buckets": 32,
      "torch_dtype": "float32",
      "transformers_version": "4.11.3",
      "use_cache": true,
      "vocab_size": 64128
    }
    
    loading weights file ./finetuned_model/pytorch_model.bin
    All model checkpoint weights were used when initializing T5ForConditionalGeneration.
    
    All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./finetuned_model.
    If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.


    ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤. 'ì¢…ë£Œ'ë¼ê³  ì…ë ¥í•˜ë©´ ëŒ€í™”ê°€ ì¢…ë£Œë©ë‹ˆë‹¤.
    ë‹¹ì‹ : ì•ˆë…•?
    ì±—ë´‡: ì•„ì˜ˆ freelyì‹­ë‹ˆê¹Œì˜ˆìš”.
    ë‹¹ì‹ : ì´ë¦„ì´ ë­ì•¼?
    ì±—ë´‡: ì•„ì˜ˆ freelyì„¸ìš”.
    ë‹¹ì‹ : ì•„ì˜ˆëŠ” ì™œ í•˜ëŠ”ê±°ì•¼?
    ì±—ë´‡: ì•„ì˜ˆ freelyì„¸ìš”.
    ë‹¹ì‹ : ìŒ,,
    ì±—ë´‡: ì•„ì˜ˆ freely ê±°ìš”ì˜ˆìš”.
    ë‹¹ì‹ : ê·¸ê²Œ ë¬´ìŠ¨ ëœ»ì´ì•¼?
    ì±—ë´‡: ë§ˆì°¬ê°€ì§€ë‹¤ì˜ˆìš”.
    ë‹¹ì‹ : ì¢…ë£Œ
    ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”!



```python

```
